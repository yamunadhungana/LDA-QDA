---
title: "Model comparision between LDA and QDA"
author: "Yamuna Dhungana"
output: 
    pdf_document:
        latex_engine: xelatex
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning=F,message=F)
```


We wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”)
based on X, last year’s percent profit.We examine a large number of companies and discover that the
mean value of X for companies that issued a dividend was $\overline{X} = 10$ , while the mean for those that didn’t was $\overline{X} = 0$. In addition, the variance of $X$ for these two sets of companies was $ˆσ2 = 36$.Finally,80% of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was 
$X = 4$ last year. Hint: Recall that the density function for a normal random variable is 
$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$$
. You will need to use Bayes’ theorem.

$$\pi _{YES} = 0.8$$
$$\pi _{NO} = 0.2$$
$$\mu _{YES} = 10$$ 
$$\mu _{N0} = 0$$ 
$${\sigma^2} = 36$$
pluging the given values to the density function here, $\pi_k$ is 0.8 and 0.2 and divident is Yes and No. 

So, using the density function to calculate $f_k(x)$

$$f_{k}(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$$
we get, 

$$f_{yes}(x)= 0.0402$$
$$f_{No}(x)= 0.0532$$

using the equation of Bayes Theorem 

$$P_{(divident=K|X=x)} = \frac{\pi_k f_k(x)}{\sum _{l=1}^{k}\pi_lf_l(x)} $$
again, plunging the value we calculated earlier in the above equation.

$$P_{yes}(4) = \frac{0.8 \times 0.04032}{0.8 \times 0.0402 + 0.2 \times 0.0532} = 0.75186$$

```{r}
library(ISLR)
library(caTools)
library(mclust)
library(MASS)
library(ggplot2)
library(knitr)

```

Using the same logic

```{r, eval=TRUE, echo=FALSE}
# pdf function
pdf <- function(x, mu_k, sigma){((sqrt(2*pi)*sigma)^-1)*(exp(-((2*sigma^2)^-1)*(x-mu_k)^2))}

sigma <- 6 # for both classes

# Type 1 where companies issued dividend
pi_1 <- .8
mu_1 <- 10

# Type 2 where companies  did not issue dividend
pi_2 <- .2
mu_2 <- 0

# Calculate probabilities based on Bayes
x <- 4
p_1 <- (pi_1*pdf(4,mu_1,sigma))/(pi_1*pdf(4,mu_1,sigma) + pi_2*pdf(4,mu_2,sigma))
p_2 <- (pi_2*pdf(4,mu_2,sigma))/(pi_1*pdf(4,mu_1,sigma) + pi_2*pdf(4,mu_2,sigma))

# rounding the numbers
p_1 <- round(p_1,4)
p_2 <- round(p_2,4)

# prediction
prediction <- data.frame(cbind(c("Dividend", "Non-Dividend"), c(p_1, p_2)))
colnames(prediction) <- c("Types", "prediction")
prediction



```

According to my analysis, 75.19% of the companies are expected to declare dividends this year, while 24.81% are not expected to do so.

We proceed to construct a model using the selected predictors from the previous assignment and fit this model using the MclustDA function from the mclust library. The same training and test sets employed previously are used in this process.
    
```{r,echo=FALSE,warning=FALSE}

# Using the previous work to start
data(Auto)
mpg01 <- rep(NA,dim(Auto)[1])
med_ian <- median(Auto$mpg)
mpg01 = ifelse(Auto$mpg<med_ian,0,1)
my_data = as.data.frame(cbind(Auto, mpg01))

# dropping mpg and names
my_data.fnl <- my_data[,c(c(2,3,4,5,6,7,8,10))]

# head(my_data)

#Splitting data into test and train  

mysplit <- sample.split(my_data.fnl, SplitRatio = 0.70)
train <- subset(my_data.fnl,mysplit==T)
test <- subset(my_data.fnl,mysplit==F)
#dim(train)

Autotrain <- train[,-8]
Autotrainclass <- train[,8]

Autotest <- test[,-8]
Autotestclass <- test[,8]



automodelDA <- MclustDA(Autotrain, Autotrainclass)
summary(automodelDA)


# BIC selection 
knitr::kable(cbind(summary.MclustDA(automodelDA)$modelName,summary.MclustDA(automodelDA)$bic),
      caption = "Best model selected by BIC", col.names=c("Model names","BIC"))
#kable(tablecom.mcl, col.names=c("Model names","BIC"))
```
   
 Similar to the prior analysis, I partitioned the dataset into training and test sets, maintaining a 70-30 ratio. I proceeded to fit the MclustDA model. Consistent with the previous approach, the response variable was binary-coded MPG (indicating whether it's greater than or equal to the median MPG). I used the same predictors as in the previous analysis, selecting them based on their correlation with 'mpg01.'

The Bayesian Information Criterion (BIC) for this model is calculated as -10568.327944618. This model exhibits an ellipsoidal shape, equal volume, and equal shape with either 3 or 4 groups.



```{r,echo=FALSE,warning=FALSE}

# To different accuracies of the model

TPR_TNR <- function(con){
  accuracy <-  round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
  TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
}
# train accuracy 
pred_train1 <-  predict(automodelDA,newdata=Autotrain)$classification
table_train_1 <- TPR_TNR(table(pred_train1, Autotrainclass))
colnames(table_train_1) <- "Mclust train set"
# table_train_1

# test error
pred_test1 <- predict(automodelDA,newdata=Autotest)$classification
table_test_1 <- TPR_TNR(table(pred_test1, Autotestclass))
colnames(table_test_1) <- "Mclust test set"
# table_test_1

tablecom.mcl <- as.data.frame(cbind(table_train_1,table_test_1))
knitr::kable(tablecom.mcl, digits = 3,
             caption = "Mclust model accuracy measures")



```
  
   I employed the identical function used in previous assignments (homework 3 and 4) to determine the test accuracies of the model. The accuracy results for both the training and testing models are displayed in Table-2.  
   
   
      
  By Specifing `modelType = "EDDA"` and run `MclustDA` again.


```{r,echo=FALSE,warning=FALSE}

model2 <- MclustDA(Autotrain, Autotrainclass, modelType = "EDDA")
summary(model2)

# BIC selection 
knitr::kable(cbind(summary.MclustDA(model2)$modelName,summary.MclustDA(model2)$bic),
      caption = "Best model selected by BIC with EDDA", col.names=c("Model names","BIC"))
#kable(tablecom.mcl, col.names=c("Model names","BIC"))
```

   
When fitting the MclustDA model using the EDDA model type, the Bayesian Information Criterion (BIC) is calculated as -11500.33. This particular model is labeled as ellipsoidal, featuring varying volume, shape, and orientation (VVV), and consists of a single group.



```{r,echo=FALSE,warning=FALSE}
## train error
pred_train2 <- predict(model2, newdata=Autotrain)$classification
table_train_3 <- TPR_TNR(table(pred_train2, Autotrainclass))
colnames(table_train_3) <- "Mclust:EDDA train set"
# table_train_3

## test error
pred_test2 <- predict(model2, newdata=Autotest)$classification
table_test_3 <- TPR_TNR(table(pred_test2,Autotestclass))
colnames(table_test_3) <- "Mclust:EDDA test set"
# table_test_3

tablecom.edda <- as.data.frame(cbind(table_train_3,table_test_3))
knitr::kable(tablecom.edda, digits = 3,
             caption = "Mclust with EDDA model accuracy measures")


```
   
   The Model accuracies are reported in the table-4 
   
    
 
```{r,echo=FALSE,warning=FALSE}

# Combined error for Mclust models

tablecom1 <- as.data.frame(cbind(table_train_1,table_test_1,table_train_3,table_test_3))
knitr::kable(tablecom1, digits = 3,
             caption = "Mclust models accuracy measures")


################################################################

# repeating my previous work
fit.log <- glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = train, family = binomial)
# summary(fit.log)


# Function for test errors

test.err <- function(cutoff,model,test){
  preds <- rep(0,dim(test)[1])
  probs <- predict(model,newdata=test, type="response")
  for(i in 1:length(probs)){
    if(probs[i]>=cutoff){
      preds[i]=1
    }
  }
  con <- table(preds,test$mpg01)
  accuracy <- round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
  TPR <- round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR <- round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
  
}

# Train error
table_train_4 <- test.err(0.5,fit.log,train)
colnames(table_train_4) <- "Logreg.model Train set"

# Test error
table_test_4 <- test.err(0.5,fit.log,test)
colnames(table_test_4) <- "Logreg.model Test set"

# Combined error for logistic regression
tablecom2 <- as.data.frame(cbind(table_train_4, table_test_4))
knitr::kable(tablecom2, digits = 3,
             caption = "Logreg Accuracy measures")

########################################################################################

# FOr lda

fit_lda = lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = train)


test.err_lqda=function(model,testt){
  trues <- testt$mpg01
  preds <- (predict(model,newdata=testt,type="response"))$class
  con <- table(preds,trues)
  #con <- table(preds,test$mpg01)
  accuracy <- round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
  TPR <- round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR <- round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
  return(as.data.frame(rbind(accuracy,TPR,TNR)))

}
# Test error
table_train_5 <- test.err_lqda(fit_lda, train)
colnames(table_train_5) <- "LDA.model Train set"

# Test error
table_test_5 <- test.err_lqda(fit_lda, test)
colnames(table_test_5) <- "LDA.model Test set"

# Combined error for LDA models
tablecom3 <- as.data.frame(cbind(table_train_5, table_test_5))
knitr::kable(tablecom3, digits = 3,
             caption = "LDA model Accuracy measures")

#################################################################################################

# For qda
fit_qda <- qda(mpg01 ~ cylinders + weight + displacement + horsepower,data=train)

# Test error for QDA
table_train_6 <- test.err_lqda(fit_qda, train)
colnames(table_train_6) <- "QDA.model Train set"

# Test error
table_test_6 <- test.err_lqda(fit_qda, test)
colnames(table_test_6) <- "QDA.model Test set"


# Combined error for QDA models
tablecom4 <- as.data.frame(cbind(table_train_6, table_test_6))
knitr::kable(tablecom4, digits = 3,
             caption = "QDA model Accuracy measures")

```
   
   In our previous assignments, we conducted several classification methods, including Logistic Regression, Linear Discriminant Analysis (LDA), and Quadratic Discriminant Analysis (QDA). In this assignment, we extended our analysis to include MclustDA with the EDDA model type. Our evaluation, as depicted in tables 5, 6, 7, and 8, indicates that MclustDA outperforms the other models in terms of accuracy. Conversely, LDA and QDA exhibit lower accuracy rates. Furthermore, MclustDA demonstrates superior True Positive Rate (TPR) and True Negative Rate (TNR) performance when compared to the other models.
    
We begin by creating a fresh set of variables derived from the original model variables. Subsequently, we construct a new model using the `MclustDA` function and repeat the process steps i to iii. We anticipate that these newly engineered variables will lead to an enhancement in error rates when compared to the previous models.

  
```{r,echo=FALSE,warning=FALSE}
# Here I am using previous variabele that shows most significance in the data

formula1 <- mpg01 ~ weight + horsepower + horsepower:weight # interaction
formula2 <- mpg01 ~ poly(weight, 2, raw = TRUE) + poly(horsepower, 2, raw = TRUE) # polynomials


hw.int <- my_data.fnl$horsepower*my_data.fnl$weight
weigh.poly <- poly(my_data.fnl$weight, 2, raw = TRUE)[,2]
hors.poly <- poly(my_data.fnl$horsepower, 2, raw = TRUE)[,2]


# Combining the new variables 
new.data <- cbind(my_data.fnl[,c(8,3,4)], hw.int,hors.poly, weigh.poly)


#Splitting data into test and train  

newdata_split <- sample.split(new.data, SplitRatio = 0.70)
newdata.train <- subset(new.data,mysplit==T)
newdata.test <- subset(new.data,mysplit==F)



a_trainX <- newdata.train[,-1]
a_trainClass <- newdata.train[,1]
a_testX <- newdata.test[,-1]
a_testClass <- newdata.test[,1]


## MclustDA

mod1 <- MclustDA(a_trainX[,1:3], a_trainClass)
summary(mod1)

mod2 <- MclustDA(a_trainX[,4:5], a_trainClass)
summary(mod2)

# BIC selection 
knitr::kable(cbind(summary.MclustDA(mod1)$modelName,summary.MclustDA(mod1)$bic, summary.MclustDA(mod2)$modelName,summary.MclustDA(mod2)$bic),
      caption = "Best model selected by BIC with two models", 
      col.names=c("Model1:model names","Model1:BIC", "MOdel2:model names", "MOdel2:BIC"))



# For model 1 
# Train set
train.predicted_1 <-  predict(mod1, newdata=a_trainX[,1:3])$classification
train.tab_1 <- TPR_TNR(table(train.predicted_1, a_trainClass))
colnames(train.tab_1) <- "Mod1.mclustDA train set"
# train.tab_1

# test set
test.predicted_1 <- predict(mod1, newdata=a_testX[,1:3])$classification
test.tab_1 <- TPR_TNR(table(test.predicted_1, a_testClass))
colnames(test.tab_1) <- "mod1.mclustDA test set"
# test.tab_1


# For model 2 
# Train set
train.predicted_2 <-  predict(mod2, newdata=a_trainX[,4:5])$classification
train.tab_2 <- TPR_TNR(table(train.predicted_2, a_trainClass))
colnames(train.tab_2) <- "Mod2.mclustDA train set"
# train.tab_2

# test set
test.predicted_2 <- predict(mod2, newdata=a_testX[,4:5])$classification
test.tab_2 <- TPR_TNR(table(test.predicted_2, a_testClass))
colnames(test.tab_2) <- "Mod2.mclustDA test set"
# test.tab_2

# Combined error for Mclust models
tablecom5 <- as.data.frame(cbind(train.tab_1, test.tab_1,train.tab_2,test.tab_2))
knitr::kable(tablecom5, digits = 3,
             caption = "MclustDA:Accuracy measures for two models")

#####################################################################################

mod1.edda <- MclustDA(a_trainX[,1:3], a_trainClass, modelType = "EDDA")
summary(mod1.edda)

mod2.edda <- MclustDA(a_trainX[,4:5], a_trainClass,modelType = "EDDA" )
summary(mod2.edda)

# BIC selection 
knitr::kable(cbind(summary.MclustDA(mod1.edda)$modelName,summary.MclustDA(mod1.edda)$bic, summary.MclustDA(mod2.edda)$modelName,summary.MclustDA(mod2.edda)$bic),
      caption = "Best model selected by BIC with two models using EDDA", 
      col.names=c("Model1:model names","Model1:BIC", "MOdel2:model names", "MOdel2:BIC"))

# For model 1 
# Train set
train.predicted_1ed <-  predict(mod1.edda, newdata=a_trainX[,1:3])$classification
train.tab_1ed <- TPR_TNR(table(train.predicted_1ed, a_trainClass))
colnames(train.tab_1ed) <- "Mod1.edda train set"
# train.tab_1

# test set
test.predicted_1ed <- predict(mod1.edda, newdata=a_testX[,1:3])$classification
test.tab_1ed <- TPR_TNR(table(test.predicted_1ed, a_testClass))
colnames(test.tab_1ed) <- "mod1.edda test set"
# test.tab_1


# For model 2 
# Train set
train.predicted_2ed <-  predict(mod2.edda, newdata=a_trainX[,4:5])$classification
train.tab_2ed <- TPR_TNR(table(train.predicted_2ed, a_trainClass))
colnames(train.tab_2ed) <- "Mod2.edda train set"
# train.tab_2

# test set
test.predicted_2ed <- predict(mod2.edda, newdata=a_testX[,4:5])$classification
test.tab_2ed <- TPR_TNR(table(test.predicted_2ed, a_testClass))
colnames(test.tab_2ed) <- "mod2.edda test set"
# test.tab_2

# Combined error for Mclust EDDA models
tablecom6 <- as.data.frame(cbind(train.tab_1ed, test.tab_1ed,train.tab_2ed,test.tab_2ed))
knitr::kable(tablecom6, digits = 3,
             caption = "MclustDA with EDDA:Accuracy measures for two models")

###########################################################################################

# First model with Logistic regression

fit_log1 <- fit.log <- glm(formula1, data = newdata.train, family = binomial)
summary(fit_log1)

# Second model with Logistic regression
fit_log2 <- fit.log <- glm(formula2, data = newdata.train, family = binomial)
summary(fit_log2)


# For model-1
# Train error
train.predicted_1log <- test.err(0.5,fit_log1,newdata.train)
colnames(train.predicted_1log) <- "mod1.Logreg Train set"

# Test error
test.predicted_1log <- test.err(0.5,fit_log1,newdata.test)
colnames(test.predicted_1log) <- "mod1.Logreg Test set"


# For model-2

# Train error
train.predicted_2log <- test.err(0.5,fit_log2,newdata.train)
colnames(train.predicted_2log) <- "mod2.Logreg Train set"

# Test error
test.predicted_2log <- test.err(0.5,fit_log2,newdata.test)
colnames(test.predicted_2log) <- "mod2.Logreg Test set"


# Combined error for Mclust EDDA models
tablecom7 <- as.data.frame(cbind(train.predicted_1log, test.predicted_1log,train.predicted_2log,test.predicted_2log))
knitr::kable(tablecom7, digits = 3,
             caption = "Logistic regression: Accuracy measures for two models")

#################################################################################################

# First model with LDA
fit_lda1 = lda(formula1, data = newdata.train)

# Second model with LDA
fit_lda2 = lda(formula2, data = newdata.train)



# MOdel-1
# Test error
table_train_lda1 <- test.err_lqda(fit_lda1, newdata.train)
colnames(table_train_lda1) <- "LDA.mod1 Train set"

# Test error
table_test_lda1 <- test.err_lqda(fit_lda1, newdata.test)
colnames(table_test_lda1) <- "LDA.mod1 Test set"




# MOdel 2
table_train_lda2 <- test.err_lqda(fit_lda2, newdata.train)
colnames(table_train_lda2) <- "LDA.mod2 Train set"

# Test error
table_test_lda2 <- test.err_lqda(fit_lda2, newdata.test)
colnames(table_test_lda2) <- "LDA.mod1 Test set"



# Combined error for Mclust EDDA models
tablecom8 <- as.data.frame(cbind(table_train_lda1, table_test_lda1,table_train_lda2,table_test_lda2))
knitr::kable(tablecom8, digits = 3,
             caption = "LDA: Accuracy measures for two models")


####################################################################################

# First model with QDA
fit_Qda1 = qda(formula1, data = newdata.train)

# Second model with LDA
fit_Qda2 = qda(formula2, data = newdata.train)



# MOdel-1
# Test error
table_train_qda1 <- test.err_lqda(fit_Qda1, newdata.train)
colnames(table_train_qda1) <- "LDA.mod1 Train set"

# Test error
table_test_qda1 <- test.err_lqda(fit_Qda1, newdata.test)
colnames(table_test_qda1) <- "QDA.mod1 Test set"




# MOdel 2
table_train_qda2 <- test.err_lqda(fit_Qda2, newdata.train)
colnames(table_train_qda2) <- "QDA.mod2 Trainset"

# Test error
table_test_qda2 <- test.err_lqda(fit_Qda2, newdata.test)
colnames(table_test_qda2) <- "QDA.mod1 Test set"



# Combined error for Mclust EDDA models
tablecom9 <- as.data.frame(cbind(table_train_qda1, table_test_qda1,table_train_qda2,table_test_qda2))
knitr::kable(tablecom9, digits = 3,
             caption = "QDA: Accuracy measures for two models")




```
   
   In prior analyses, I identified that the variables 'cylinders,' 'weight,' 'displacement,' and 'horsepower' were strongly associated with the 'mpg01' variable. However, when conducting logistic regression, I found that the p-values for 'weight' and 'horsepower' were statistically significant. Consequently, I opted to focus on these two variables for interactions and polynomial transformations.

I created an interaction term, `mpg01 ~ weight + horsepower + horsepower:weight`, and a polynomial term, `mpg01 ~ poly(weight, 2, raw = TRUE) + poly(horsepower, 2, raw = TRUE)`. Subsequently, I fitted models using MclustDA, MclustDA with EDDA, Logistic Regression, Linear Discriminant Analysis (LDA), and Quadratic Discriminant Analysis (QDA) with these variables.

From the MclustDA and MclustDA with EDDA models, the Bayesian Information Criterion (BIC) values are presented in Table-9 and Table-11. Model-1 exhibited a higher BIC than the second model and is considered a better fit. The model names for Model-1 are 'ellipsoidal, equal volume and equal shape (EEV)' and 'ellipsoidal, equal shape (VEV).' In contrast, the model names for the second model are 'ellipsoidal, equal orientation (new models in mclust version >= 5.0.0) (VVE)' and 'ellipsoidal, equal orientation (new models in mclust version >= 5.0.0) (VEE).' 

For MclustDA with EDDA, Model-1 also outperforms the second model with a higher BIC. Both models share the model name 'ellipsoidal, varying volume, shape, and orientation (VVV).' 

I proceeded to fit all the models with the same interaction and polynomial terms, and calculated accuracy measures. The accuracy of these models is presented in the table. The accuracy varies from 89.9% to 82.99%. The MclustDA model achieves the highest accuracy. Notably, the introduction of these new variables did not lead to an improvement in model accuracy.


```{r,echo=FALSE,warning=FALSE}
# https://stats.stackexchange.com/questions/280344/multiple-polynomial-regression-versus-gam
# https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html
# https://stackoverflow.com/questions/60951077/rename-a-single-column-in-kable-table
# Cami's solution video for homework-5


```
